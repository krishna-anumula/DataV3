{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from itertools import compress\n",
    "import nltk\n",
    "import en_core_web_lg\n",
    "nlp_spacy = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output_combined.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('article_data_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1088, 7)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = data_1.truncate(after=1087)\n",
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "505it [00:00, 21396.05it/s]\n",
      "1088it [00:00, 29708.11it/s]\n"
     ]
    }
   ],
   "source": [
    "snippets_list = []\n",
    "for idx, row in tqdm(data.iterrows()):\n",
    "        temp_dict = dict()\n",
    "        temp_dict['title'] = row['title']\n",
    "        temp_dict['url'] = row['url']\n",
    "        temp_dict['body'] = row['body']\n",
    "        temp_dict['company_name'] = row['company_name']\n",
    "        snippets_list.append(temp_dict)\n",
    "for idx, row in tqdm(data_1.iterrows()):\n",
    "        temp_dict = dict()\n",
    "        temp_dict['title'] = row['title']\n",
    "        temp_dict['url'] = row['url']\n",
    "        temp_dict['body'] = row['data']\n",
    "        temp_dict['company_name'] = row['company_name']\n",
    "        snippets_list.append(temp_dict)\n",
    "\n",
    "data_new = pd.DataFrame(snippets_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1593, 4), Index(['title', 'url', 'body', 'company_name'], dtype='object'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.shape, data_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           Intellectual Property Rights During -- and Aft...\n",
       "url             https:\\/\\/www.mddionline.com\\/legal\\/intellect...\n",
       "body            Manufacturers came to the rescue to quickly de...\n",
       "company_name                                   ford_motor_company\n",
       "Name: 1592, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.to_csv(\"data_bef_prep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aft_prep = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'article_id', 'company_name', 'title', 'id'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aft_prep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = ['alphabet','google','facebook','ITV','Pearson','BT group','BT','Vodafone group','vodafone','burberry group','burberry','nike','nike inc','compass group',\n",
    "'restaurant group','trg','amazon','amazon in','booking holdings','kingfisher','marks and spencer group','marks and spencer','marks & spencer group','marks & spencer',\n",
    "'next plc','tjx','tjx companies','sainsbury','tesco','british american tobacco','coca-cola','coca cola','diageo','imperial brands','pepsico',\n",
    "'PHILIP MORRIS INTERNATIONAL','philip morris','TATE AND LYLE','TATE & LYLE','PROCTER & GAMBLE','PROCTER and GAMBLE','p&g','RECKITT BENCKISER GROUP','RECKITT BENCKISER',\n",
    "'RECKITT','RB','unilever','BAKER HUGHES','BP','CABOT OIL & GAS CORP','coterra energy','CHEVRON CORP','CHEVRON','EOG RESOURCES','eog','EXXON MOBIL','EXXONMOBIL',\n",
    "'HALLIBURTON','PHILLIPS','PIONEER NATURAL RESOURCE','PIONEER NATURAL RESOURCES','VALERO ENERGY','VALERO','BANK OF AMERICA','BARCLAYS','CITIGROUP','CITI',\n",
    "'FIRST REPUBLIC BANK','FIRST REPUBLIC','JPMORGAN CHASE','JPMORGAN','JP MORGAN','J.P. MORGAN','LLOYDS BANKING GROUP','LLOYDS BANK','NATWEST GROUP','NATWEST',\n",
    "'STANDARD CHARTERED','STANDARD CHARTERED PLC','WELLS FARGO','3I GROUP','3I','AMERICAN EXPRESS','BERKSHIRE HATHAWAY INC','BERKSHIRE HATHAWAY','BERKSHIRE',\n",
    "'HATHAWAY','CME GROUP','HARGREAVES LANSDOWN','LONDON STOCK EXCHANGE GROUP','LSEG','LONDON STOCK EXCHANGE','AVIVA','DIRECT LINE INSURANCE','DIRECT LINE group',\n",
    "'DIRECT LINE','LEGAL AND GENERAL GROUP','LEGAL AND GENERAL','legal & general','PHOENIX GROUP','phoenix','PROGRESSIVE CORP','PROGRESSIVE CORPORATION','PROGRESSIVE','PRUDENTIAL','ANTHEM',\n",
    "'BOSTON SCIENTIFIC','CONVATEC GROUP','CONVATEC','DANAHER CORPORATION','DANAHER CORP','DANAHER','SMITH AND NEPHEW','SMITH & NEPHEW','ASTRAZENECA','GLAXOSMITHKLINE',\n",
    "'GSK','ILLUMINA','JOHNSON & JOHNSON',\"JOHNSON's\",'J&J','THERMO FISHER SCIENTIFIC','THERMO FISHER','ASHTEAD GROUP','ASHTEAD','BAE SYSTEMS','BUNZL','ELECTROCOMPONENTS',\n",
    "'FERGUSON PLC','FERGUSON','IMI PLC','IMI','MELROSE INDUSTRIES','MELROSE','QINETIQ GROUP','QINETIQ','RAYTHEON TECHNOLOGIES','RAYTHEON','SMITHS GROUP','SMITHS',\n",
    "'HAYS PLC','HAYS','IHS MARKIT','RELX group','RELX','RENTOKIL INITIAL','RENTOKIL','SERCO GROUP','SERCO','FIRSTGROUP','GO-AHEAD GROUP','GO-AHEAD','ROYAL MAIL group',\n",
    "'ROYAL MAIL','ADVANCED MICRO DEVICES','AMD','QORVO','TEXAS INSTRUMENT','TEXAS INSTRUMENTS','ADOBE','ATLASSIAN','ATLASSIAN CORPORATION','AVEVA GROUP','AVEVA',\n",
    "'CAPITA GROUP','CAPITA','CTS','Cognizant','FDM GROUP HOLDINGS','FDM GROUP','FDM','FIDELITY NATIONAL INFORMATION SERVICES','FNI Services','GODADDY','INTUIT',\n",
    "'MASTERCARD','MICROSOFT CORP','MICROSOFT CORPORATION','MICROSOFT','OKTA','PAYCHEX','PAYPAL','SNOWFLAKE','SS AND C TECHNOLOGIES','SS&C TECHNOLOGIES','SS&C',\n",
    "'VISA INC','VISA','ZOOM VIDEO COMMUNICATIONS','ZOOM VIDEO','ZOOM COMMUNICATIONS','AMPHENOL CORP','AMPHENOL CORPORATION','AMPHENOL','CDW','CISCO SYSTEMS','CISCO',\n",
    "'HP','Hewlett-Packard','MOTOROLA SOLUTIONS','MOTOROLA','QUALCOMM','SPIRENT COMMUNICATIONS','SPIRENT','ANGLO AMERICAN','BHP GROUP','BHP','CRODA INTERNATIONAL','CRODA',\n",
    "'ESSENTRA','INTERNATIONAL FLAVORS & FRAGRANCES','IFF','JOHNSON MATTHEY','JMAT','MARTIN MARIETTA MATERIALS INC','MARTIN MARIETTA MATERIALS','MONDI','MONDI GROUP',\n",
    "'NEWMONT','NEWMONT CORPORATION','SMITH (DS)','DS Smith','ASSURA REIT','ASSURA FINANCING','ASSURA','CAPITAL & COUNTIES PROPERTIES','CAPITAL & COUNTIES','CROWN CASTLE',\n",
    "'CROWN CASTLE INTERNATIONAL CORP','CROWN CASTLE INTERNATIONAL CORPORATION','EQUINIX REIT','EQUINIX','EQUITY LIFESTYLE PROPERTIES','ELS','LAND SECURITIES GROUP',\n",
    "'LANDSEC','LONDONMETRIC PROPERTY','LMP','SBA COMMUNICATIONS','SBA COMMUNICATIONS CORPORATION','SBAC','SEGRO PLC','SEGRO','SHAFTESBURY PLC','SHAFTESBURY',\n",
    "'SUN COMMUNITIES','AMERICAN ELECTRIC POWER','american electric','AEP','CENTRICA','EXELON CORPORATION','EXELON','NATIONAL GRID','NEXTERA ENERGY','NEXTERA','PUBLIC SERVICE ENTERPRISE GROUP',\n",
    "'PSEG','UNITED UTILITIES GROUP','UNITED UTILITIES','WEC ENERGY GROUP','WEC ENERGY','SSE PLC','GENERAL MOTORS','LEAR CORPORATION','LEAR','LEAR CORP','STARBUCKS',\n",
    "'EBAY','TESLA INC','TESLA',\"MCDONALD'S CORP\",\"MCDONALD'S\",'WALMART','WALMART CORPORATE','COLGATE-PALMOLIVE','COLGATE','3M','3M SCIENCE','ROLLS-ROYCE HOLDINGS','ROLLS-ROYCE',\n",
    "'EQUIFAX','FEDEX CORP','FEDEX CORPORATION','FEDEX','UBER TECHNOLOGIES','UBER','GENERAL ELECTRIC','GE','LOCKHEED MARTIN CORPORATION','LOCKHEED MARTIN','NVIDIA CORPORATION',\n",
    "'NVIDIA','APPLE INC','APPLE','XCEL ENERGY','AUTOHOME','BAIDU','KAKAKU','NETEASE','NINTENDO','NINTENDO LTD','NIPPON TELEVISION','NIPPON TV','TENCENT HOLDINGS',\n",
    "'TENCENT','BHARTI AIRTEL','AIRTEL','KDDI CORP','KDDI CORPORATION','KDDI','NIPPON TELEGRAPH AND TELEPHONE CORPORATION','NTT CORP','SOFTBANK CORP','SOFTBANK GROUP',\n",
    "'SOFTBANK','AISIN CORP','AISIN','BRIDGESTONE CORPORATION','BRIDGESTONE GROUP','BRIDGESTONE','DONGFENG MOTOR GROUP','HONDA MOTOR COMPANY','HONDA','ISUZU MOTORS LTD',\n",
    "'ISUZU MOTORS','ISUZU','MAHINDRA AND MAHINDRA LTD','MAHINDRA & MAHINDRA LTD','MAHINDRA & MAHINDRA','Mahindra GROUP','SUBARU CORP','SUBARU CORPORATION','SUBARU',\n",
    "'SUMITOMO ELECTRIC INDUSTRIES','SUMITOMO ELECTRIC','SUZUKI MOTOR CORP','SUZUKI MOTOR CORPORATION','SUZUKI MOTOR','SUZUKI','TATA MOTORS LTD','TATA MOTORS',\n",
    "'TOYOTA MOTOR CORP','TOYOTA MOTOR CORPORATION','TOYOTA','XPENG INC','XPENG INC.','XPENG','YAMAHA MOTOR COMPANY','YAMAHA MOTORS','YAMAHA','YAMAHA MOTOR CO., LTD.',\n",
    "'YAMAHA MOTOR','PARKER-HANNIFIN CORP','PARKER-HANNIFIN','PARKER HANNIFIN','Nestlé India','Nestlé','Nestle','PFIZER INC','PFIZER','msci','msci inc','salesforce','itochu',\n",
    "'komatsu','airbnb','sage','expedia','bajaj finance','twilio','whitbread','otis','japan tobacco','daifuku','ACCIONA','ALDI','AMP Capital','ASDA','AXA','Actelion','Air Liquide','Aramco','Aurora Cannabis','Aviva','AVIVA PLC','Bank Hapoalim BM','BestBuy','Biffa','Big Society Capital','Blackrock','Boeing','British Airways','Cambridge Glasshouse Company','Castrol','credit Suisse','DWS','Deutsche Bank','Dewan Housing Finance Corporation Ltd','Discovery Limited','Dove','Drax','E.ON','ESSECO','Eastman Kodak','Enel','Eni','FMC','Flipkart','Ford', 'Ford Motor Company','Freedom Foods Group Ltd', 'GM+Shell','Generali','Goldman Sachs','Granville','H&M','HAZAMA ANDO CORP','HS2','Hanson','Heathrow','Heineken','ICICI Bank','ING','Infosys','Jhonson Controls','John Lewis','KLM Airlines','KPMG','Kao data','Kobe Steel','Konkola Copper Mines','LG Polymers','LeasePlan','Lego','Lombard Odier','Luckin','Luckin Coffee Inc', 'Mars','Mercedes Benz','Metlife','Monzo Bank', 'Morrisons','Anglo American','Salzgitter','Arrival','First Bus','EIB','Enel','Flynn','Primark','Shell', 'Rolls-Royce', 'Tevva', 'Vattenfall','Violia','Carbon Clean','Nishimatsu Construction Co., Ltd.', 'Nokia','O2', 'OVO Energy','Polestar','SQUAD','Stena Bulk', 'Takeda', 'Target','Toshiba IT-Services Corp', 'Total','Turing Pharma','Valeant Pharma','Yes Bank Limited','Zhenhua','abu_dhabi_national_oil_company','agricultural bank of china','alibaba', 'Alibaba Group Holding Limited', 'Alibaba Group Holding', 'Alibaba.com','alibaba_group',]\n",
    "\n",
    "\n",
    "comp_list = [each_string.lower() for each_string in comp_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_org(text):\n",
    "        lst=[]\n",
    "        doc = nlp_spacy(text)\n",
    "        for word in doc.ents:\n",
    "            if(word.label_=='ORG'):\n",
    "                low_txt = word.text.lower()\n",
    "                lst.append(low_txt)\n",
    "        return lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_comp_title(snippet_company,title_company):\n",
    "    snippet_company = snippet_company[:-1]\n",
    "    comp_lst=snippet_company.split(\",\")\n",
    "    \n",
    "    if(all(elem == comp_lst[0] for elem in comp_lst) and comp_lst[0]==title_company):     #all the companies found in snippet level are belonging to company found at article\n",
    "        return '4-1',title_company\n",
    "    \n",
    "    elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):              #any one of the companies found in snippet level are belonging to company found at article\n",
    "        return '4-2',title_company\n",
    "    \n",
    "    else:                                                                           #company found at snippet level are not belonging to company found at article\n",
    "        return '4-3','None'\n",
    "\n",
    "def mul_comp_title(snippet_company,title_company):\n",
    "    snippet_company = snippet_company[:-1]\n",
    "    comp_lst=snippet_company.split(\",\")\n",
    "    \n",
    "    if(all(elem == comp_lst[0] for elem in comp_lst) and any(map(lambda v: v in title_company.split(\",\"), comp_lst))):      #all the companies found in snippet level are belonging to any one of the company found at article\n",
    "        return '7-1',snippet_company.split(\",\")[0]\n",
    "    \n",
    "    elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):                                                      #any one of the companies in snippet level are belonging to any one of the company found at article\n",
    "        lst = list(map(lambda v: v in title_company.split(\",\"), comp_lst))\n",
    "        return '7-2',','. join(set(compress(comp_lst, lst)))\n",
    "    \n",
    "    else:                                                                                                                   #company found at snippet level are not belonging to any company found at article\n",
    "        return '7-3','None'\n",
    "\n",
    "\n",
    "def tag(snippet,title):\n",
    "    k=0\n",
    "    #title = title\n",
    "    #snippet = data_list[id-(key*15)+1]\n",
    "    title_company=\"\"\n",
    "    snippet_company=\"\"\n",
    "\n",
    "    lst_art = ent_org(title) #get ORG tagged texts for Article title\n",
    "    lst_snip = ent_org(snippet) #get ORG tagged texts for snippets\n",
    "\n",
    "    for company in comp_list:          #Check for ORG texts in master list\n",
    "        if(company.lower() in lst_art):\n",
    "            k=1\n",
    "            title_company=title_company+company+\",\"\n",
    "        if(company.lower() in lst_snip):\n",
    "            snippet_company=snippet_company+company+\",\"\n",
    "\n",
    "    #Code for the snippets, where article title do not have company name\n",
    "    if(k==0):\n",
    "        \n",
    "        snippet_company = snippet_company[:-1]\n",
    "        \n",
    "        if not snippet_company:     #Getting coref data, since snippet don't have any company\n",
    "            return '1',\"coref\",\"None\"\n",
    "        else:                                          #having company in snippet\n",
    "            return '3',snippet_company,\"None\"\n",
    "        \n",
    "    #Code for the snippets, where article title have company name\n",
    "    else:\n",
    "        title_company = title_company[:-1]\n",
    "\n",
    "        if(len(title_company.split(\",\"))==1):       #Article title having single company\n",
    "            \n",
    "            if(snippet_company):                    #snippet having company name\n",
    "                snippet_company = snippet_company[:-1]\n",
    "                comp_lst=snippet_company.split(\",\")\n",
    "                \n",
    "                if(all(elem == comp_lst[0] for elem in comp_lst) and comp_lst[0]==title_company):     #all the companies found in snippet level are belonging to company found at article\n",
    "                    return '4-1',title_company,title_company\n",
    "                \n",
    "                elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):              #any one of the companies found in snippet level are belonging to company found at article\n",
    "                    return '4-2',title_company,title_company\n",
    "                \n",
    "                else:                                                                           #company found at snippet level are not belonging to company found at article\n",
    "                    return '4-3','None',title_company\n",
    "                #return one_comp_title(snippet_company,title_company),title_company\n",
    "\n",
    "            else:\n",
    "                return '6',title_company,title_company\n",
    "        \n",
    "        else:                                       #Article title having multiple companies\n",
    "        \n",
    "            if(snippet_company):                     #Snippet have company name\n",
    "                snippet_company = snippet_company[:-1]\n",
    "                comp_lst=snippet_company.split(\",\")\n",
    "                \n",
    "                if(all(elem == comp_lst[0] for elem in comp_lst) and any(map(lambda v: v in title_company.split(\",\"), comp_lst))):      #all the companies found in snippet level are belonging to any one of the company found at article\n",
    "                    return '7-1',snippet_company.split(\",\")[0],title_company\n",
    "                \n",
    "                elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):                                                      #any one of the companies in snippet level are belonging to any one of the company found at article\n",
    "                    lst = list(map(lambda v: v in title_company.split(\",\"), comp_lst))\n",
    "                    return '7-2',','. join(set(compress(comp_lst, lst))),title_company\n",
    "                \n",
    "                else:                                                                                                                   #company found at snippet level are not belonging to any company found at article\n",
    "                    return '7-3','None',title_company\n",
    "                #return mul_comp_title(snippet_company,title_company),title_company\n",
    "\n",
    "            else:\n",
    "                return '9',title_company,title_company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64138"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_aft_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64138it [10:58, 97.40it/s] \n"
     ]
    }
   ],
   "source": [
    "no_tit_no_snip=0\n",
    "no_tit=0\n",
    "no_tit_snip=0\n",
    "sin_tit_no_snip=0\n",
    "yes_tit=0\n",
    "sin_tit=0\n",
    "mul_tit_no_snip=0\n",
    "mul_tit=0\n",
    "sin_tit_all_snip=0\n",
    "sin_tit_any_snip=0\n",
    "sin_tit_oth_snip=0\n",
    "mul_tit_all_snip=0\n",
    "mul_tit_any_snip=0\n",
    "mul_tit_oth_snip=0\n",
    "final_snippend_list=[]\n",
    "\n",
    "for idx, row in tqdm(data_aft_prep.iterrows()):\n",
    "    if(str(row['text'])!='nan' and str(row['title'])!='nan'):\n",
    "        value,snippet_org, title_org = tag(row['text'],row['title'])\n",
    "    else:\n",
    "        value='nan'\n",
    "        snippet_org='nan'\n",
    "        title_org='nan'\n",
    "    temp_dict=dict()\n",
    "    temp_dict['article_id'] = row['article_id']\n",
    "    temp_dict['title'] = row['title']\n",
    "    temp_dict['snippet'] = row['text']\n",
    "    temp_dict['company_name'] = row['company_name']\n",
    "    #temp_dict['snippet_coref'] = coref\n",
    "    temp_dict['snippet_org'] = snippet_org\n",
    "    #temp_dict['snippet_coref_org'] = snippet_coref_org\n",
    "    temp_dict['title_org'] = title_org\n",
    "    if(snippet_org == title_org):\n",
    "        temp_dict['tit_sni_same'] = 1\n",
    "    else:\n",
    "        temp_dict['tit_sni_same'] = 0\n",
    "    #temp_dict['coref_tit_sni_same'] = bool2\n",
    "    temp_dict['id'] = row['id']\n",
    "    temp_dict['value'] = value\n",
    "    if(value == '1'):\n",
    "        no_tit_no_snip+=1\n",
    "        no_tit+=1\n",
    "    elif(value == '3'):\n",
    "        no_tit_snip+=1\n",
    "        no_tit+=1\n",
    "    elif(value == '6'):\n",
    "        sin_tit_no_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '9'):\n",
    "        mul_tit_no_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    elif(value == '4-1'):\n",
    "        sin_tit_all_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '4-2'):\n",
    "        sin_tit_any_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '4-3'):\n",
    "        sin_tit_oth_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '7-1'):\n",
    "        mul_tit_all_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    elif(value == '7-2'):\n",
    "        mul_tit_any_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    elif(value == '7-3'):\n",
    "        mul_tit_oth_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    \n",
    "    final_snippend_list.append(temp_dict)\n",
    "snippet_df = pd.DataFrame(final_snippend_list) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_df.to_csv(\"emperical_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of sentences :  64138\n",
      "10700\n",
      "\n",
      "Percentage of sentences with company in title : 16.68%\n",
      "\n",
      "53400\n",
      "\n",
      "Percentage of sentences with no company in title : 83.26%\n",
      "\n",
      "9533\n",
      "\n",
      "Percentage of sentences with single company in title : 14.86%\n",
      "\n",
      "1167\n",
      "\n",
      "Percentage of sentences with multiple companies in title : 1.82%\n",
      "\n",
      "50561\n",
      "\n",
      "Percentage of sentences with no company in title as well as snippet : 78.83%\n",
      "\n",
      "2839\n",
      "\n",
      "Percentage of sentences with no company in title and company in snippet : 4.43%\n",
      "\n",
      "7173\n",
      "\n",
      "Percentage of sentences with single company in title and no company in snippet : 11.18%\n",
      "\n",
      "2071\n",
      "\n",
      "Percentage of sentences with single company in title and snippet have title company : 3.23%\n",
      "\n",
      "83\n",
      "\n",
      "Percentage of sentences with single company in title and one of the companies in snippet is title company : 0.13%\n",
      "\n",
      "206\n",
      "\n",
      "Percentage of sentences with single company in title and snippet company is not matching title company : 0.32%\n",
      "\n",
      "973\n",
      "\n",
      "Percentage of sentences with multiple companies in title and no company in snippet : 1.52%\n",
      "\n",
      "126\n",
      "\n",
      "Percentage of sentences with multiple companies in title and snippet having one of the company in title : 0.2%\n",
      "\n",
      "32\n",
      "\n",
      "Percentage of sentences with multiple companies in title and any one of the companies in snippet is same as one of the title company : 0.05%\n",
      "\n",
      "36\n",
      "\n",
      "Percentage of sentences with multiple companies in title and snippet company is matching none of the title company : 0.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of sentences : \",len(data_aft_prep))\n",
    "print(yes_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with company in title : \"+str(round((yes_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(no_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with no company in title : \"+str(round((no_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title : \"+str(round((sin_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title : \"+str(round((mul_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(no_tit_no_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with no company in title as well as snippet : \"+str(round((no_tit_no_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(no_tit_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with no company in title and company in snippet : \"+str(round((no_tit_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_no_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and no company in snippet : \"+str(round((sin_tit_no_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_all_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and snippet have title company : \"+str(round((sin_tit_all_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_any_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and one of the companies in snippet is title company : \"+str(round((sin_tit_any_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_oth_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and snippet company is not matching title company : \"+str(round((sin_tit_oth_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_no_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and no company in snippet : \"+str(round((mul_tit_no_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_all_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and snippet having one of the company in title : \"+str(round((mul_tit_all_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_any_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and any one of the companies in snippet is same as one of the title company : \"+str(round((mul_tit_any_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_oth_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and snippet company is matching none of the title company : \"+str(round((mul_tit_oth_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('emperical_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = result[result.value=='3'].snippet_org# or result.value=='4-3' or result.value=='3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "lst = list(set(lst))\n",
    "count=0\n",
    "for i in lst:\n",
    "    lst_spl = i.split(\",\")\n",
    "    if(len(lst_spl)>1):\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_comp_title(snippet_company,title_company):\n",
    "    snippet_company = snippet_company[:-1]\n",
    "    comp_lst=snippet_company.split(\",\")\n",
    "    \n",
    "    if(all(elem == comp_lst[0] for elem in comp_lst) and comp_lst[0]==title_company):     #all the companies found in snippet level are belonging to company found at article\n",
    "        return '4-1',title_company\n",
    "    \n",
    "    elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):              #any one of the companies found in snippet level are belonging to company found at article\n",
    "        return '4-2',title_company\n",
    "    \n",
    "    else:                                                                           #company found at snippet level are not belonging to company found at article\n",
    "        return '4-3','None'\n",
    "\n",
    "def mul_comp_title(snippet_company,title_company):\n",
    "    snippet_company = snippet_company[:-1]\n",
    "    comp_lst=snippet_company.split(\",\")\n",
    "    \n",
    "    if(all(elem == comp_lst[0] for elem in comp_lst) and any(map(lambda v: v in title_company.split(\",\"), comp_lst))):      #all the companies found in snippet level are belonging to any one of the company found at article\n",
    "        return '7-1',snippet_company.split(\",\")[0]\n",
    "    \n",
    "    elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):                                                      #any one of the companies in snippet level are belonging to any one of the company found at article\n",
    "        lst = list(map(lambda v: v in title_company.split(\",\"), comp_lst))\n",
    "        return '7-2',','. join(set(compress(comp_lst, lst)))\n",
    "    \n",
    "    else:                                                                                                                   #company found at snippet level are not belonging to any company found at article\n",
    "        return '7-3','None'\n",
    "\n",
    "\n",
    "def tag(snippet,title,coref_flag):\n",
    "    k=0\n",
    "    #title = title\n",
    "    #snippet = data_list[id-(key*15)+1]\n",
    "    title_company=\"\"\n",
    "    snippet_company=\"\"\n",
    "\n",
    "    lst_art = ent_org(title) #get ORG tagged texts for Article title\n",
    "    lst_snip = ent_org(snippet) #get ORG tagged texts for snippets\n",
    "\n",
    "    for company in comp_list:          #Check for ORG texts in master list\n",
    "        if(company.lower() in lst_art):\n",
    "            k=1\n",
    "            title_company=title_company+company+\",\"\n",
    "        if(company.lower() in lst_snip):\n",
    "            snippet_company=snippet_company+company+\",\"\n",
    "\n",
    "    #Code for the snippets, where article title do not have company name\n",
    "    if(k==0):\n",
    "        \n",
    "        snippet_company = snippet_company[:-1]\n",
    "        \n",
    "        if not snippet_company and coref_flag==0:     #Getting coref data, since snippet don't have any company\n",
    "            return '1',\"coref\",\"None\"\n",
    "        elif not snippet_company and coref_flag==1:\n",
    "                return '2',\"None\",\"None\"\n",
    "        else:                                          #having company in snippet\n",
    "            return '3',snippet_company,\"None\"\n",
    "        \n",
    "    #Code for the snippets, where article title have company name\n",
    "    else:\n",
    "        title_company = title_company[:-1]\n",
    "\n",
    "        if(len(title_company.split(\",\"))==1):       #Article title having single company\n",
    "            \n",
    "            if(snippet_company):                    #snippet having company name\n",
    "                snippet_company = snippet_company[:-1]\n",
    "                comp_lst=snippet_company.split(\",\")\n",
    "                \n",
    "                if(all(elem == comp_lst[0] for elem in comp_lst) and comp_lst[0]==title_company):     #all the companies found in snippet level are belonging to company found at article\n",
    "                    return '4-1',title_company,title_company\n",
    "                \n",
    "                elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):              #any one of the companies found in snippet level are belonging to company found at article\n",
    "                    return '4-2',title_company,title_company\n",
    "                \n",
    "                else:                                                                           #company found at snippet level are not belonging to company found at article\n",
    "                    return '4-3','None',title_company\n",
    "                #return one_comp_title(snippet_company,title_company),title_company\n",
    "            elif coref_flag==0:                                   #Snippet don't have company name\n",
    "                    return '5',\"coref\",title_company\n",
    "            else:\n",
    "                return '6',title_company,title_company\n",
    "        \n",
    "        else:                                       #Article title having multiple companies\n",
    "        \n",
    "            if(snippet_company):                     #Snippet have company name\n",
    "                snippet_company = snippet_company[:-1]\n",
    "                comp_lst=snippet_company.split(\",\")\n",
    "                \n",
    "                if(all(elem == comp_lst[0] for elem in comp_lst) and any(map(lambda v: v in title_company.split(\",\"), comp_lst))):      #all the companies found in snippet level are belonging to any one of the company found at article\n",
    "                    return '7-1',snippet_company.split(\",\")[0],title_company\n",
    "                \n",
    "                elif(any(map(lambda v: v in title_company.split(\",\"), comp_lst))):                                                      #any one of the companies in snippet level are belonging to any one of the company found at article\n",
    "                    lst = list(map(lambda v: v in title_company.split(\",\"), comp_lst))\n",
    "                    return '7-2',','. join(set(compress(comp_lst, lst))),title_company\n",
    "                \n",
    "                else:                                                                                                                   #company found at snippet level are not belonging to any company found at article\n",
    "                    return '7-3','None',title_company\n",
    "                #return mul_comp_title(snippet_company,title_company),title_company\n",
    "            elif coref_flag==0:                                   #Snippet don't have company name\n",
    "                    return '8',\"coref\",title_company\n",
    "            else:\n",
    "                return '9',title_company,title_company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "        \n",
    "        final_token = document[coref[1]]\n",
    "        if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "            resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "        else:\n",
    "            resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "        for i in range(coref[0] + 1, coref[1] + 1):\n",
    "            resolved[i] = \"\"\n",
    "        \n",
    "        return resolved\n",
    "\n",
    "\n",
    "def original_replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        mention_start, mention_end = cluster[0][0], cluster[0][1] + 1\n",
    "        mention_span = document[mention_start:mention_end]\n",
    "\n",
    "        for coref in cluster[1:]:\n",
    "            core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n",
    "\n",
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
    "    #used to know the exact clusters which are having atleast one noun or propn\n",
    "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "    return span_noun_indices\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "    \n",
    "    for i in range(len(noun_indices)):\n",
    "        head_start, head_end = cluster[noun_indices[i]]\n",
    "        head_span = doc[head_start:head_end+1]\n",
    "        if(str(head_span).lower() in comp_list):\n",
    "            return head_span, [head_start, head_end]\n",
    "    return \"null\",[-1,-1]\n",
    "    \n",
    "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    \n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "        if noun_indices:\n",
    "            for i in range(len(noun_indices)):\n",
    "                head_start, head_end = cluster[noun_indices[i]]\n",
    "                head_span = document[head_start:head_end+1]\n",
    "                #print(head_span)\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "            \n",
    "            if(mention!=[-1,-1]):\n",
    "                #print(\"mention span : \",mention_span)\n",
    "                for coref in cluster:\n",
    "                    if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                        #print(is_containing_other_spans(coref, all_spans))\n",
    "                        core_logic_part(document, coref, resolved, mention_span)\n",
    "    return \"\".join(resolved)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n12844/.local/lib/python3.8/site-packages/allennlp/tango/__init__.py:17: UserWarning: AllenNLP Tango is an experimental API and parts of it might change or disappear every time we release a new version.\n",
      "  warnings.warn(\n",
      "/home/n12844/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2021-11-29 09:26:45,669 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
      "2021-11-29 09:26:46,157 - INFO - allennlp.common.file_utils - cache of https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz is up-to-date\n",
      "2021-11-29 09:26:46,159 - INFO - allennlp.models.archival - loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /home/n12844/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
      "2021-11-29 09:26:46,160 - INFO - allennlp.models.archival - extracting archive file /home/n12844/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /tmp/tmpzlhqyapz\n",
      "2021-11-29 09:26:54,140 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
      "2021-11-29 09:26:54,141 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
      "2021-11-29 09:26:54,142 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
      "2021-11-29 09:26:54,142 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
      "2021-11-29 09:26:54,142 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
      "2021-11-29 09:26:54,142 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
      "2021-11-29 09:26:54,144 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "2021-11-29 09:26:54,145 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "2021-11-29 09:26:54,145 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "2021-11-29 09:26:54,146 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "2021-11-29 09:26:54,146 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
      "2021-11-29 09:26:54,147 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
      "2021-11-29 09:26:54,147 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
      "2021-11-29 09:26:54,148 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "2021-11-29 09:27:08,661 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "2021-11-29 09:27:08,662 - INFO - allennlp.common.params - dataset_reader.max_sentences = 110\n",
      "2021-11-29 09:27:08,662 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
      "2021-11-29 09:27:08,663 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
      "2021-11-29 09:27:08,663 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
      "2021-11-29 09:27:08,664 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
      "2021-11-29 09:27:08,664 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
      "2021-11-29 09:27:08,664 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
      "2021-11-29 09:27:08,665 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
      "2021-11-29 09:27:08,666 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "2021-11-29 09:27:08,666 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
      "2021-11-29 09:27:08,667 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
      "2021-11-29 09:27:08,667 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "2021-11-29 09:27:08,667 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
      "2021-11-29 09:27:08,668 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
      "2021-11-29 09:27:08,668 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
      "2021-11-29 09:27:08,668 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
      "2021-11-29 09:27:08,670 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
      "2021-11-29 09:27:08,670 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
      "2021-11-29 09:27:08,670 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
      "2021-11-29 09:27:08,670 - INFO - allennlp.common.params - type = from_instances\n",
      "2021-11-29 09:27:08,671 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpzlhqyapz/vocabulary.\n",
      "2021-11-29 09:27:08,671 - INFO - allennlp.common.params - model.type = coref\n",
      "2021-11-29 09:27:08,672 - INFO - allennlp.common.params - model.regularizer = None\n",
      "2021-11-29 09:27:08,672 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
      "2021-11-29 09:27:08,672 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
      "2021-11-29 09:27:08,673 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
      "2021-11-29 09:27:08,674 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
      "2021-11-29 09:27:08,674 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "2021-11-29 09:27:08,675 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
      "2021-11-29 09:27:08,675 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
      "2021-11-29 09:27:08,675 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
      "2021-11-29 09:27:08,676 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
      "2021-11-29 09:27:08,676 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
      "2021-11-29 09:27:08,676 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
      "2021-11-29 09:27:08,676 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
      "2021-11-29 09:27:08,677 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
      "2021-11-29 09:27:08,677 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
      "2021-11-29 09:27:08,677 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
      "2021-11-29 09:27:08,677 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
      "2021-11-29 09:27:08,677 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
      "2021-11-29 09:27:08,678 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-11-29 09:27:15,100 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
      "2021-11-29 09:27:15,101 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
      "2021-11-29 09:27:15,101 - INFO - allennlp.common.params - model.context_layer.input_dim = 1024\n",
      "2021-11-29 09:27:15,102 - INFO - allennlp.common.params - model.mention_feedforward.type = ref\n",
      "2021-11-29 09:27:15,104 - INFO - allennlp.common.params - model.mention_feedforward.input_dim = 3092\n",
      "2021-11-29 09:27:15,105 - INFO - allennlp.common.params - model.mention_feedforward.num_layers = 2\n",
      "2021-11-29 09:27:15,105 - INFO - allennlp.common.params - model.mention_feedforward.hidden_dims = 1500\n",
      "2021-11-29 09:27:15,107 - INFO - allennlp.common.params - model.mention_feedforward.activations = relu\n",
      "2021-11-29 09:27:15,108 - INFO - allennlp.common.params - type = relu\n",
      "2021-11-29 09:27:15,108 - INFO - allennlp.common.params - type = relu\n",
      "2021-11-29 09:27:15,109 - INFO - allennlp.common.params - type = relu\n",
      "2021-11-29 09:27:15,109 - INFO - allennlp.common.params - model.mention_feedforward.dropout = 0.3\n",
      "2021-11-29 09:27:15,139 - INFO - allennlp.common.params - model.antecedent_feedforward.type = ref\n",
      "2021-11-29 09:27:15,140 - INFO - allennlp.common.params - model.antecedent_feedforward.input_dim = 9296\n",
      "2021-11-29 09:27:15,141 - INFO - allennlp.common.params - model.antecedent_feedforward.num_layers = 2\n",
      "2021-11-29 09:27:15,141 - INFO - allennlp.common.params - model.antecedent_feedforward.hidden_dims = 1500\n",
      "2021-11-29 09:27:15,142 - INFO - allennlp.common.params - model.antecedent_feedforward.activations = relu\n",
      "2021-11-29 09:27:15,142 - INFO - allennlp.common.params - type = relu\n",
      "2021-11-29 09:27:15,142 - INFO - allennlp.common.params - type = relu\n",
      "2021-11-29 09:27:15,143 - INFO - allennlp.common.params - type = relu\n",
      "2021-11-29 09:27:15,143 - INFO - allennlp.common.params - model.antecedent_feedforward.dropout = 0.3\n",
      "2021-11-29 09:27:15,209 - INFO - allennlp.common.params - model.feature_size = 20\n",
      "2021-11-29 09:27:15,210 - INFO - allennlp.common.params - model.max_span_width = 30\n",
      "2021-11-29 09:27:15,210 - INFO - allennlp.common.params - model.spans_per_word = 0.4\n",
      "2021-11-29 09:27:15,211 - INFO - allennlp.common.params - model.max_antecedents = 50\n",
      "2021-11-29 09:27:15,211 - INFO - allennlp.common.params - model.coarse_to_fine = True\n",
      "2021-11-29 09:27:15,211 - INFO - allennlp.common.params - model.inference_order = 2\n",
      "2021-11-29 09:27:15,212 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
      "2021-11-29 09:27:15,212 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7fb104175f10>\n",
      "2021-11-29 09:27:15,252 - INFO - allennlp.nn.initializers - Initializing parameters\n",
      "2021-11-29 09:27:15,255 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2021-11-29 09:27:15,255 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
      "2021-11-29 09:27:15,256 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.weight\n",
      "2021-11-29 09:27:15,256 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
      "2021-11-29 09:27:15,256 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.weight\n",
      "2021-11-29 09:27:15,256 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
      "2021-11-29 09:27:15,257 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.weight\n",
      "2021-11-29 09:27:15,257 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.bias\n",
      "2021-11-29 09:27:15,257 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.weight\n",
      "2021-11-29 09:27:15,257 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.bias\n",
      "2021-11-29 09:27:15,258 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.weight\n",
      "2021-11-29 09:27:15,258 - INFO - allennlp.nn.initializers -    _distance_embedding.weight\n",
      "2021-11-29 09:27:15,258 - INFO - allennlp.nn.initializers -    _endpoint_span_extractor._span_width_embedding.weight\n",
      "2021-11-29 09:27:15,258 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
      "2021-11-29 09:27:15,259 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.weight\n",
      "2021-11-29 09:27:15,259 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
      "2021-11-29 09:27:15,259 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.weight\n",
      "2021-11-29 09:27:15,259 - INFO - allennlp.nn.initializers -    _mention_scorer._module.bias\n",
      "2021-11-29 09:27:15,259 - INFO - allennlp.nn.initializers -    _mention_scorer._module.weight\n",
      "2021-11-29 09:27:15,260 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.bias\n",
      "2021-11-29 09:27:15,260 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.weight\n",
      "2021-11-29 09:27:15,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
      "2021-11-29 09:27:15,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
      "2021-11-29 09:27:15,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
      "2021-11-29 09:27:15,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
      "2021-11-29 09:27:15,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
      "2021-11-29 09:27:15,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
      "2021-11-29 09:27:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
      "2021-11-29 09:27:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
      "2021-11-29 09:27:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
      "2021-11-29 09:27:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
      "2021-11-29 09:27:15,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
      "2021-11-29 09:27:15,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
      "2021-11-29 09:27:15,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
      "2021-11-29 09:27:15,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
      "2021-11-29 09:27:15,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
      "2021-11-29 09:27:15,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
      "2021-11-29 09:27:15,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
      "2021-11-29 09:27:15,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
      "2021-11-29 09:27:15,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
      "2021-11-29 09:27:15,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,268 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,268 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
      "2021-11-29 09:27:15,268 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
      "2021-11-29 09:27:15,268 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,268 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,269 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,269 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,269 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
      "2021-11-29 09:27:15,269 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
      "2021-11-29 09:27:15,269 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
      "2021-11-29 09:27:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
      "2021-11-29 09:27:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
      "2021-11-29 09:27:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
      "2021-11-29 09:27:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,270 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,271 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,271 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
      "2021-11-29 09:27:15,271 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
      "2021-11-29 09:27:15,271 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,271 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
      "2021-11-29 09:27:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
      "2021-11-29 09:27:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
      "2021-11-29 09:27:15,272 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
      "2021-11-29 09:27:15,273 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
      "2021-11-29 09:27:15,273 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
      "2021-11-29 09:27:15,273 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,273 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,273 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
      "2021-11-29 09:27:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
      "2021-11-29 09:27:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,274 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,275 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,275 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
      "2021-11-29 09:27:15,275 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
      "2021-11-29 09:27:15,275 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
      "2021-11-29 09:27:15,275 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
      "2021-11-29 09:27:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
      "2021-11-29 09:27:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
      "2021-11-29 09:27:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,276 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,277 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
      "2021-11-29 09:27:15,277 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
      "2021-11-29 09:27:15,277 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,277 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,277 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,278 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,278 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
      "2021-11-29 09:27:15,278 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
      "2021-11-29 09:27:15,278 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
      "2021-11-29 09:27:15,278 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
      "2021-11-29 09:27:15,284 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
      "2021-11-29 09:27:15,284 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
      "2021-11-29 09:27:15,284 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,284 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,284 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,285 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,285 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
      "2021-11-29 09:27:15,285 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
      "2021-11-29 09:27:15,285 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,286 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,286 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,286 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,286 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
      "2021-11-29 09:27:15,287 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
      "2021-11-29 09:27:15,287 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
      "2021-11-29 09:27:15,287 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
      "2021-11-29 09:27:15,287 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
      "2021-11-29 09:27:15,288 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
      "2021-11-29 09:27:15,288 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,290 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,291 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,291 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,291 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
      "2021-11-29 09:27:15,292 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
      "2021-11-29 09:27:15,292 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,292 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,293 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,293 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,293 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
      "2021-11-29 09:27:15,294 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
      "2021-11-29 09:27:15,294 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
      "2021-11-29 09:27:15,294 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
      "2021-11-29 09:27:15,295 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
      "2021-11-29 09:27:15,296 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
      "2021-11-29 09:27:15,296 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,297 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,297 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,297 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,297 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
      "2021-11-29 09:27:15,298 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
      "2021-11-29 09:27:15,298 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,298 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,298 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,298 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,299 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
      "2021-11-29 09:27:15,299 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
      "2021-11-29 09:27:15,299 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
      "2021-11-29 09:27:15,299 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
      "2021-11-29 09:27:15,299 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
      "2021-11-29 09:27:15,300 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
      "2021-11-29 09:27:15,300 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,300 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,300 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,300 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,301 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
      "2021-11-29 09:27:15,301 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
      "2021-11-29 09:27:15,301 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,301 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,302 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,302 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,302 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
      "2021-11-29 09:27:15,302 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
      "2021-11-29 09:27:15,302 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
      "2021-11-29 09:27:15,303 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
      "2021-11-29 09:27:15,303 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
      "2021-11-29 09:27:15,303 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
      "2021-11-29 09:27:15,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,451 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,452 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,452 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,452 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
      "2021-11-29 09:27:15,453 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
      "2021-11-29 09:27:15,453 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,457 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,459 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,459 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,460 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
      "2021-11-29 09:27:15,460 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
      "2021-11-29 09:27:15,460 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
      "2021-11-29 09:27:15,461 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
      "2021-11-29 09:27:15,461 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
      "2021-11-29 09:27:15,461 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
      "2021-11-29 09:27:15,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
      "2021-11-29 09:27:15,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
      "2021-11-29 09:27:15,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,472 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,472 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,472 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
      "2021-11-29 09:27:15,473 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
      "2021-11-29 09:27:15,473 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
      "2021-11-29 09:27:15,474 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
      "2021-11-29 09:27:15,474 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
      "2021-11-29 09:27:15,474 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
      "2021-11-29 09:27:15,475 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,475 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,475 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
      "2021-11-29 09:27:15,478 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
      "2021-11-29 09:27:15,479 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,479 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,479 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,479 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,480 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
      "2021-11-29 09:27:15,480 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
      "2021-11-29 09:27:15,480 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
      "2021-11-29 09:27:15,480 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
      "2021-11-29 09:27:15,481 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
      "2021-11-29 09:27:15,481 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
      "2021-11-29 09:27:15,481 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,481 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,482 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,482 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,482 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
      "2021-11-29 09:27:15,483 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
      "2021-11-29 09:27:15,483 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,483 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,483 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,484 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,484 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
      "2021-11-29 09:27:15,484 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
      "2021-11-29 09:27:15,484 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
      "2021-11-29 09:27:15,485 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
      "2021-11-29 09:27:15,485 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
      "2021-11-29 09:27:15,485 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
      "2021-11-29 09:27:15,486 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,486 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,486 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,487 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,487 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
      "2021-11-29 09:27:15,487 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
      "2021-11-29 09:27:15,488 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,488 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,488 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,489 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,489 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
      "2021-11-29 09:27:15,489 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
      "2021-11-29 09:27:15,490 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
      "2021-11-29 09:27:15,490 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
      "2021-11-29 09:27:15,491 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
      "2021-11-29 09:27:15,491 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
      "2021-11-29 09:27:15,491 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,492 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
      "2021-11-29 09:27:15,493 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
      "2021-11-29 09:27:15,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,494 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,495 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
      "2021-11-29 09:27:15,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
      "2021-11-29 09:27:15,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
      "2021-11-29 09:27:15,496 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
      "2021-11-29 09:27:15,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
      "2021-11-29 09:27:15,497 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
      "2021-11-29 09:27:15,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,498 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,499 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
      "2021-11-29 09:27:15,501 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
      "2021-11-29 09:27:15,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,506 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
      "2021-11-29 09:27:15,507 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
      "2021-11-29 09:27:15,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
      "2021-11-29 09:27:15,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
      "2021-11-29 09:27:15,508 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
      "2021-11-29 09:27:15,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
      "2021-11-29 09:27:15,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,509 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,510 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
      "2021-11-29 09:27:15,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
      "2021-11-29 09:27:15,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,511 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,512 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,512 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
      "2021-11-29 09:27:15,512 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
      "2021-11-29 09:27:15,513 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
      "2021-11-29 09:27:15,513 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
      "2021-11-29 09:27:15,513 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
      "2021-11-29 09:27:15,514 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
      "2021-11-29 09:27:15,514 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,514 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,515 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,515 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,515 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
      "2021-11-29 09:27:15,515 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
      "2021-11-29 09:27:15,516 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,516 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,516 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,517 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,517 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
      "2021-11-29 09:27:15,517 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
      "2021-11-29 09:27:15,517 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
      "2021-11-29 09:27:15,518 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
      "2021-11-29 09:27:15,518 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
      "2021-11-29 09:27:15,518 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
      "2021-11-29 09:27:15,519 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,519 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,519 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,520 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,520 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
      "2021-11-29 09:27:15,520 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
      "2021-11-29 09:27:15,520 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,521 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,521 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,521 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,522 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
      "2021-11-29 09:27:15,522 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
      "2021-11-29 09:27:15,523 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
      "2021-11-29 09:27:15,523 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
      "2021-11-29 09:27:15,523 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
      "2021-11-29 09:27:15,523 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
      "2021-11-29 09:27:15,524 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,524 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,524 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,524 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,525 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
      "2021-11-29 09:27:15,525 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
      "2021-11-29 09:27:15,525 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,528 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,529 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,529 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,529 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
      "2021-11-29 09:27:15,529 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
      "2021-11-29 09:27:15,530 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
      "2021-11-29 09:27:15,530 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
      "2021-11-29 09:27:15,530 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
      "2021-11-29 09:27:15,530 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
      "2021-11-29 09:27:15,530 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,531 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,531 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,531 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,531 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
      "2021-11-29 09:27:15,531 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
      "2021-11-29 09:27:15,532 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,533 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,533 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,533 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,533 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
      "2021-11-29 09:27:15,534 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
      "2021-11-29 09:27:15,534 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
      "2021-11-29 09:27:15,534 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
      "2021-11-29 09:27:15,534 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
      "2021-11-29 09:27:15,534 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
      "2021-11-29 09:27:15,535 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,535 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,535 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,535 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,536 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
      "2021-11-29 09:27:15,536 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
      "2021-11-29 09:27:15,536 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,536 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,536 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,537 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,537 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
      "2021-11-29 09:27:15,537 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
      "2021-11-29 09:27:15,537 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
      "2021-11-29 09:27:15,538 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
      "2021-11-29 09:27:15,538 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
      "2021-11-29 09:27:15,538 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
      "2021-11-29 09:27:15,538 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,538 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,539 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,539 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,539 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
      "2021-11-29 09:27:15,539 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
      "2021-11-29 09:27:15,539 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,540 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,540 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
      "2021-11-29 09:27:15,540 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
      "2021-11-29 09:27:15,540 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
      "2021-11-29 09:27:15,540 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
      "2021-11-29 09:27:15,541 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
      "2021-11-29 09:27:15,541 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
      "2021-11-29 09:27:15,541 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
      "2021-11-29 09:27:15,541 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
      "2021-11-29 09:27:15,541 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
      "2021-11-29 09:27:15,542 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
      "2021-11-29 09:27:15,542 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2021-11-29 09:27:15,542 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2021-11-29 09:27:15,542 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
      "2021-11-29 09:27:15,542 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
      "2021-11-29 09:27:15,543 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
      "2021-11-29 09:27:15,543 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
      "2021-11-29 09:27:15,550 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2021-11-29 09:27:15,551 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
      "2021-11-29 09:27:16,213 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpzlhqyapz\n"
     ]
    }
   ],
   "source": [
    "predictor,nlp = utils.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coref(txt):\n",
    "        clusters = predictor.predict(txt)['clusters']\n",
    "        tot_art = nlp(txt)\n",
    "        allen1 = improved_replace_corefs(tot_art, clusters).strip()\n",
    "        if(len(allen1.split(\"\\n\"))>1):\n",
    "                return(allen1.split(\"\\n\")[1])\n",
    "        else:\n",
    "                return(txt.split(\"\\n\")[1])\n",
    "\n",
    "        #return allen1.split(\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set completed\n",
      "set completed\n"
     ]
    }
   ],
   "source": [
    "no_tit_no_snip=0\n",
    "no_tit=0\n",
    "no_tit_snip=0\n",
    "sin_tit_no_snip=0\n",
    "yes_tit=0\n",
    "sin_tit=0\n",
    "mul_tit_no_snip=0\n",
    "mul_tit=0\n",
    "sin_tit_all_snip=0\n",
    "sin_tit_any_snip=0\n",
    "sin_tit_oth_snip=0\n",
    "mul_tit_all_snip=0\n",
    "mul_tit_any_snip=0\n",
    "mul_tit_oth_snip=0\n",
    "final_snippend_list=[]\n",
    "\n",
    "for idx, row in data_aft_prep.iterrows():\n",
    "    if(idx>48000):\n",
    "        coref=0\n",
    "        snippet_org=\"\"\n",
    "        if(str(row['text'])!='nan' and str(row['title'])!='nan'):\n",
    "            value,snippet_org, title_org = tag(row['text'],row['title'],0)\n",
    "        else:\n",
    "            value='nan'\n",
    "            snippet_org='nan'\n",
    "            title_org='nan'\n",
    "        if(snippet_org==\"coref\"):\n",
    "            allen = get_coref(row['title']+\".\"+\"\\n\"+row['text'])\n",
    "            coref=1\n",
    "            value,snippet_org, title_org = tag(allen,row['title'],1)\n",
    "        temp_dict=dict()\n",
    "        temp_dict['article_id'] = row['article_id']\n",
    "        temp_dict['title'] = row['title']\n",
    "        temp_dict['snippet'] = row['text']\n",
    "        temp_dict['company_name'] = row['company_name']\n",
    "        temp_dict['title_org'] = title_org\n",
    "        if(coref):\n",
    "            temp_dict['snippet_coref'] = allen\n",
    "            temp_dict['snippet_org'] = \"None\"\n",
    "            temp_dict['snippet_coref_org'] = snippet_org\n",
    "            if(snippet_org == title_org):\n",
    "                temp_dict['coref_tit_sni_same'] = 1\n",
    "            else:\n",
    "                temp_dict['coref_tit_sni_same'] = 0\n",
    "            temp_dict['tit_sni_same'] = \"None\"\n",
    "            temp_dict['is_coref'] = 1\n",
    "        else:\n",
    "            temp_dict['snippet_coref'] = \"None\"\n",
    "            temp_dict['snippet_org'] = snippet_org\n",
    "            temp_dict['snippet_coref_org'] = \"None\"\n",
    "            if(snippet_org == title_org):\n",
    "                temp_dict['tit_sni_same'] = 1\n",
    "            else:\n",
    "                temp_dict['tit_sni_same'] = 0\n",
    "            temp_dict['coref_tit_sni_same'] = \"None\"\n",
    "            temp_dict['is_coref'] = 0\n",
    "        temp_dict['id'] = row['id']\n",
    "        temp_dict['value'] = value\n",
    "        if(value == '1'):\n",
    "            no_tit_no_snip+=1\n",
    "            no_tit+=1\n",
    "        elif(value == '3'):\n",
    "            no_tit_snip+=1\n",
    "            no_tit+=1\n",
    "        elif(value == '6'):\n",
    "            sin_tit_no_snip+=1\n",
    "            yes_tit+=1\n",
    "            sin_tit+=1\n",
    "        elif(value == '9'):\n",
    "            mul_tit_no_snip+=1\n",
    "            yes_tit+=1\n",
    "            mul_tit+=1\n",
    "        elif(value == '4-1'):\n",
    "            sin_tit_all_snip+=1\n",
    "            yes_tit+=1\n",
    "            sin_tit+=1\n",
    "        elif(value == '4-2'):\n",
    "            sin_tit_any_snip+=1\n",
    "            yes_tit+=1\n",
    "            sin_tit+=1\n",
    "        elif(value == '4-3'):\n",
    "            sin_tit_oth_snip+=1\n",
    "            yes_tit+=1\n",
    "            sin_tit+=1\n",
    "        elif(value == '7-1'):\n",
    "            mul_tit_all_snip+=1\n",
    "            yes_tit+=1\n",
    "            mul_tit+=1\n",
    "        elif(value == '7-2'):\n",
    "            mul_tit_any_snip+=1\n",
    "            yes_tit+=1\n",
    "            mul_tit+=1\n",
    "        elif(value == '7-3'):\n",
    "            mul_tit_oth_snip+=1\n",
    "            yes_tit+=1\n",
    "            mul_tit+=1\n",
    "        \n",
    "        final_snippend_list.append(temp_dict)\n",
    "        if(idx%6000==0):\n",
    "            snippet_df = pd.DataFrame(final_snippend_list) \n",
    "            snippet_df.to_csv(\"emperical_result_coref_\"+str(idx)+\".csv\")\n",
    "            final_snippend_list=[]\n",
    "            print(\"set completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_df = pd.DataFrame(final_snippend_list) \n",
    "snippet_df.to_csv(\"emperical_result_coref_\"+str(idx)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = pd.read_csv(\"emperical_result_coref_0.csv\")\n",
    "data_6000 = pd.read_csv(\"emperical_result_coref_6000.csv\")\n",
    "data_12000 = pd.read_csv(\"emperical_result_coref_12000.csv\")\n",
    "data_18000 = pd.read_csv(\"emperical_result_coref_18000.csv\")\n",
    "data_24000 = pd.read_csv(\"emperical_result_coref_24000.csv\")\n",
    "data_30000 = pd.read_csv(\"emperical_result_coref_30000.csv\")\n",
    "data_36000 = pd.read_csv(\"emperical_result_coref_36000.csv\")\n",
    "data_42000 = pd.read_csv(\"emperical_result_coref_42000.csv\")\n",
    "data_48000 = pd.read_csv(\"emperical_result_coref_48000.csv\")\n",
    "data_54000 = pd.read_csv(\"emperical_result_coref_54000.csv\")\n",
    "data_60000 = pd.read_csv(\"emperical_result_coref_60000.csv\")\n",
    "data_64137 = pd.read_csv(\"emperical_result_coref_64137.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64138, 14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_aft_coref = data_0.append(data_6000)\n",
    "data_aft_coref = data_aft_coref.append(data_12000)\n",
    "data_aft_coref = data_aft_coref.append(data_18000)\n",
    "data_aft_coref = data_aft_coref.append(data_24000)\n",
    "data_aft_coref = data_aft_coref.append(data_30000)\n",
    "data_aft_coref = data_aft_coref.append(data_36000)\n",
    "data_aft_coref = data_aft_coref.append(data_42000)\n",
    "data_aft_coref = data_aft_coref.append(data_48000)\n",
    "data_aft_coref = data_aft_coref.append(data_54000)\n",
    "data_aft_coref = data_aft_coref.append(data_60000)\n",
    "data_aft_coref = data_aft_coref.append(data_64137)\n",
    "data_aft_coref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3139\n",
      "2800\n",
      "83\n",
      "6439\n",
      "149\n",
      "32\n",
      "949\n"
     ]
    }
   ],
   "source": [
    "print(data_aft_coref[data_aft_coref['value'] == \"3\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"4-1\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"4-2\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"6\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"7-1\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"7-2\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"9\"]['value'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50260\n",
      "3139\n",
      "2800\n",
      "83\n",
      "211\n",
      "0\n",
      "6439\n",
      "149\n",
      "32\n",
      "37\n",
      "0\n",
      "949\n",
      "*********\n",
      "58707\n",
      "58377\n",
      "329\n",
      "5431\n",
      "*********\n",
      "5431\n",
      "2205\n",
      "3226\n",
      "58707\n"
     ]
    }
   ],
   "source": [
    "print(data_aft_coref[data_aft_coref['value'] == \"1\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"2\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"3\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"4-1\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"4-2\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"4-3\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"5\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"6\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"7-1\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"7-2\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"7-3\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"8\"]['value'].count())\n",
    "print(data_aft_coref[data_aft_coref['value'] == \"9\"]['value'].count())\n",
    "print(\"*********\")\n",
    "print(data_aft_coref[data_aft_coref['is_coref'] == 1]['is_coref'].count())\n",
    "print(data_aft_coref[data_aft_coref['coref_tit_sni_same'] == \"1\"]['coref_tit_sni_same'].count())\n",
    "print(data_aft_coref[data_aft_coref['coref_tit_sni_same'] == \"0\"]['coref_tit_sni_same'].count())\n",
    "print(data_aft_coref[data_aft_coref['coref_tit_sni_same'] == \"None\"]['coref_tit_sni_same'].count())\n",
    "print(\"*********\")\n",
    "print(data_aft_coref[data_aft_coref['is_coref'] == 0]['is_coref'].count())\n",
    "print(data_aft_coref[data_aft_coref['tit_sni_same'] == \"1\"]['tit_sni_same'].count())\n",
    "print(data_aft_coref[data_aft_coref['tit_sni_same'] == \"0\"]['tit_sni_same'].count())\n",
    "print(data_aft_coref[data_aft_coref['tit_sni_same'] == \"None\"]['tit_sni_same'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of sentences :  64138\n",
      "10176\n",
      "\n",
      "Percentage of sentences with company in title : 15.87%\n",
      "\n",
      "2572\n",
      "\n",
      "Percentage of sentences with no company in title : 4.01%\n",
      "\n",
      "9009\n",
      "\n",
      "Percentage of sentences with single company in title : 14.05%\n",
      "\n",
      "1167\n",
      "\n",
      "Percentage of sentences with multiple companies in title : 1.82%\n",
      "\n",
      "0\n",
      "\n",
      "Percentage of sentences with no company in title as well as snippet : 0.0%\n",
      "\n",
      "2572\n",
      "\n",
      "Percentage of sentences with no company in title and company in snippet : 4.01%\n",
      "\n",
      "6047\n",
      "\n",
      "Percentage of sentences with single company in title and no company in snippet : 9.43%\n",
      "\n",
      "2677\n",
      "\n",
      "Percentage of sentences with single company in title and snippet have title company : 4.17%\n",
      "\n",
      "80\n",
      "\n",
      "Percentage of sentences with single company in title and one of the companies in snippet is title company : 0.12%\n",
      "\n",
      "205\n",
      "\n",
      "Percentage of sentences with single company in title and snippet company is not matching title company : 0.32%\n",
      "\n",
      "949\n",
      "\n",
      "Percentage of sentences with multiple companies in title and no company in snippet : 1.48%\n",
      "\n",
      "149\n",
      "\n",
      "Percentage of sentences with multiple companies in title and snippet having one of the company in title : 0.23%\n",
      "\n",
      "32\n",
      "\n",
      "Percentage of sentences with multiple companies in title and any one of the companies in snippet is same as one of the title company : 0.05%\n",
      "\n",
      "37\n",
      "\n",
      "Percentage of sentences with multiple companies in title and snippet company is matching none of the title company : 0.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of sentences : \",len(data_aft_prep))\n",
    "print(yes_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with company in title : \"+str(round((yes_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(no_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with no company in title : \"+str(round((no_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title : \"+str(round((sin_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title : \"+str(round((mul_tit/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(no_tit_no_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with no company in title as well as snippet : \"+str(round((no_tit_no_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(no_tit_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with no company in title and company in snippet : \"+str(round((no_tit_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_no_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and no company in snippet : \"+str(round((sin_tit_no_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_all_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and snippet have title company : \"+str(round((sin_tit_all_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_any_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and one of the companies in snippet is title company : \"+str(round((sin_tit_any_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(sin_tit_oth_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with single company in title and snippet company is not matching title company : \"+str(round((sin_tit_oth_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_no_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and no company in snippet : \"+str(round((mul_tit_no_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_all_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and snippet having one of the company in title : \"+str(round((mul_tit_all_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_any_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and any one of the companies in snippet is same as one of the title company : \"+str(round((mul_tit_any_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()\n",
    "print(mul_tit_oth_snip)\n",
    "print()\n",
    "print(\"Percentage of sentences with multiple companies in title and snippet company is matching none of the title company : \"+str(round((mul_tit_oth_snip/len(data_aft_prep))*100,2))+\"%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tit_no_snip=0\n",
    "no_tit=0\n",
    "no_tit_snip=0\n",
    "sin_tit_no_snip=0\n",
    "yes_tit=0\n",
    "sin_tit=0\n",
    "mul_tit_no_snip=0\n",
    "mul_tit=0\n",
    "sin_tit_all_snip=0\n",
    "sin_tit_any_snip=0\n",
    "sin_tit_oth_snip=0\n",
    "mul_tit_all_snip=0\n",
    "mul_tit_any_snip=0\n",
    "mul_tit_oth_snip=0\n",
    "final_snippend_list=[]\n",
    "\n",
    "grouped_df = data_aft_prep.groupby(['article_id'])\n",
    "for name,group in grouped_df:\n",
    "    \n",
    "\n",
    "for idx, row in tqdm(data_aft_prep.iterrows()):\n",
    "    coref=0\n",
    "    snippet_org=\"\"\n",
    "    if(str(row['text'])!='nan' and str(row['title'])!='nan'):\n",
    "        value,snippet_org, title_org = tag(row['text'],row['title'],0)\n",
    "    else:\n",
    "        value='nan'\n",
    "        snippet_org='nan'\n",
    "        title_org='nan'\n",
    "    if(snippet_org==\"coref\"):\n",
    "        allen = get_coref(row['title']+\"\\n\"+row['text'])\n",
    "        coref=1\n",
    "        value,snippet_org, title_org = tag(allen,row['title'],1)\n",
    "    temp_dict=dict()\n",
    "    temp_dict['article_id'] = row['article_id']\n",
    "    temp_dict['title'] = row['title']\n",
    "    temp_dict['snippet'] = row['text']\n",
    "    temp_dict['company_name'] = row['company_name']\n",
    "    temp_dict['title_org'] = title_org\n",
    "    if(coref):\n",
    "        temp_dict['snippet_coref'] = allen\n",
    "        temp_dict['snippet_org'] = \"None\"\n",
    "        temp_dict['snippet_coref_org'] = snippet_org\n",
    "        if(snippet_org == title_org):\n",
    "            temp_dict['coref_tit_sni_same'] = 1\n",
    "        else:\n",
    "            temp_dict['coref_tit_sni_same'] = 0\n",
    "        temp_dict['tit_sni_same'] = \"None\"\n",
    "        temp_dict['is_coref'] = 1\n",
    "    else:\n",
    "        temp_dict['snippet_coref'] = \"None\"\n",
    "        temp_dict['snippet_org'] = snippet_org\n",
    "        temp_dict['snippet_coref_org'] = \"None\"\n",
    "        if(snippet_org == title_org):\n",
    "            temp_dict['tit_sni_same'] = 1\n",
    "        else:\n",
    "            temp_dict['tit_sni_same'] = 0\n",
    "        temp_dict['coref_tit_sni_same'] = \"None\"\n",
    "        temp_dict['is_coref'] = 0\n",
    "    temp_dict['id'] = row['id']\n",
    "    temp_dict['value'] = value\n",
    "    if(value == '1'):\n",
    "        no_tit_no_snip+=1\n",
    "        no_tit+=1\n",
    "    elif(value == '3'):\n",
    "        no_tit_snip+=1\n",
    "        no_tit+=1\n",
    "    elif(value == '6'):\n",
    "        sin_tit_no_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '9'):\n",
    "        mul_tit_no_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    elif(value == '4-1'):\n",
    "        sin_tit_all_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '4-2'):\n",
    "        sin_tit_any_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '4-3'):\n",
    "        sin_tit_oth_snip+=1\n",
    "        yes_tit+=1\n",
    "        sin_tit+=1\n",
    "    elif(value == '7-1'):\n",
    "        mul_tit_all_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    elif(value == '7-2'):\n",
    "        mul_tit_any_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    elif(value == '7-3'):\n",
    "        mul_tit_oth_snip+=1\n",
    "        yes_tit+=1\n",
    "        mul_tit+=1\n",
    "    \n",
    "    final_snippend_list.append(temp_dict)\n",
    "snippet_df = pd.DataFrame(final_snippend_list) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('trainV3.csv')\n",
    "data_dev = pd.read_csv('devV3.csv')\n",
    "data_test = pd.read_csv('testV3.csv')\n",
    "data_aug = pd.read_csv('total_aug_data.csv')\n",
    "data_trainaug = pd.read_csv('trainPaugV3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2083, 12), (412, 12), (462, 12), (4904, 15), (6977, 14))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_dev.shape, data_test.shape, data_aug.shape, data_trainaug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['index', 'doccano_auto_id', 'article_id', 'prev_id', 'company_name',\n",
       "        'article_title', 'text', 'ESG_label', 'difficulty', 'doccano_label',\n",
       "        'sub_category', 'version'],\n",
       "       dtype='object'),\n",
       " Index(['index', 'doccano_auto_id', 'article_id', 'prev_id', 'company_name',\n",
       "        'article_title', 'text', 'ESG_label', 'difficulty', 'doccano_label',\n",
       "        'sub_category', 'version'],\n",
       "       dtype='object'),\n",
       " Index(['index', 'doccano_auto_id', 'article_id', 'prev_id', 'company_name',\n",
       "        'article_title', 'text', 'ESG_label', 'difficulty', 'doccano_label',\n",
       "        'sub_category', 'version'],\n",
       "       dtype='object'),\n",
       " Index(['index', 'doccano_auto_id', 'article_id', 'prev_id', 'company_name',\n",
       "        'article_title', 'text', 'ESG_label', 'difficulty', 'doccano_label',\n",
       "        'sub_category', 'version', 'aug_type', 'ID_val', 'delete'],\n",
       "       dtype='object'),\n",
       " Index(['index', 'doccano_auto_id', 'article_id', 'prev_id', 'company_name',\n",
       "        'article_title', 'text', 'ESG_label', 'difficulty', 'doccano_label',\n",
       "        'sub_category', 'version', 'ID_val', 'is_aug'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns, data_dev.columns, data_test.columns, data_aug.columns, data_trainaug.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/n12844/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stb(df):\n",
    "    \n",
    "    df = df.drop_duplicates('title')\n",
    "    df = df.drop_duplicates('body')\n",
    "    df['article_id'] = range(0, len(df))\n",
    "    df = df.sort_values('title')\n",
    "    df = df.sort_values('article_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new2 = delete_stb(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2208, 9)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>rel</th>\n",
       "      <th>hit_count</th>\n",
       "      <th>probable_esg_type</th>\n",
       "      <th>company_name</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>Fracking machinery at a site operated by Range...</td>\n",
       "      <td>228</td>\n",
       "      <td>9</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>OTIS WORLDWIDE CORP (OTIS) Q1 2021 Earnings Ca...</td>\n",
       "      <td>https://www.fool.com/earnings/call-transcripts...</td>\n",
       "      <td>Good morning, and welcome to Otis's First Quar...</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>OTIS WORLDWIDE CORP (OTIS) Q4 2020 Earnings Ca...</td>\n",
       "      <td>https://www.fool.com/earnings/call-transcripts...</td>\n",
       "      <td>Good morning and welcome to Otis Fourth Quarte...</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>﻿ Otis Unveils New Generation of Digitally Nat...</td>\n",
       "      <td>https://www.intellasia.net/%EF%BB%BFotis-unvei...</td>\n",
       "      <td>- Gen3 elevator adds built-in IoT benefits to ...</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>Otis Unveils New Generation of Digitally Nativ...</td>\n",
       "      <td>https://www.intellasia.net/otis-unveils-new-ge...</td>\n",
       "      <td>- Gen3 elevator adds built-in IoT benefits to ...</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>Otis Worldwide : Unveils New Generation of Dig...</td>\n",
       "      <td>https://www.marketscreener.com//quote/stock/OT...</td>\n",
       "      <td>FARMINGTON, Conn., June 8, 2021 /PRNewswire/ -...</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-20</td>\n",
       "      <td>Central Maine business briefs: OTIS Federal Cr...</td>\n",
       "      <td>https://www.centralmaine.com/2021/02/20/centra...</td>\n",
       "      <td>JAY -- OTIS Federal Credit Union raised a tota...</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-05-24</td>\n",
       "      <td>Otis College of Art and Design's Fashion Thesi...</td>\n",
       "      <td>https://wwd.com/fashion-news/fashion-features/...</td>\n",
       "      <td>Students graduating from the fashion program a...</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-04-29</td>\n",
       "      <td>Today in History: Aretha Franklin's cover of O...</td>\n",
       "      <td>https://santamariatimes.com/lifestyles/today-i...</td>\n",
       "      <td>Today is Thursday, April 29, the 119th day of ...</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-08-23</td>\n",
       "      <td>Why Supermodel Carré Otis Is Taking on This To...</td>\n",
       "      <td>https://www.thedailybeast.com/supermodel-carre...</td>\n",
       "      <td>Carré Otis wants justice, but she also wants c...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>all</td>\n",
       "      <td>otis</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                              title  \\\n",
       "0   2021-03-30  Joseph Otis Minott: Biden must prioritize heal...   \n",
       "1   2021-04-26  OTIS WORLDWIDE CORP (OTIS) Q1 2021 Earnings Ca...   \n",
       "2   2021-02-01  OTIS WORLDWIDE CORP (OTIS) Q4 2020 Earnings Ca...   \n",
       "3   2021-06-08  ﻿ Otis Unveils New Generation of Digitally Nat...   \n",
       "4   2021-06-08  Otis Unveils New Generation of Digitally Nativ...   \n",
       "6   2021-06-08  Otis Worldwide : Unveils New Generation of Dig...   \n",
       "7   2021-02-20  Central Maine business briefs: OTIS Federal Cr...   \n",
       "8   2021-05-24  Otis College of Art and Design's Fashion Thesi...   \n",
       "9   2021-04-29  Today in History: Aretha Franklin's cover of O...   \n",
       "10  2021-08-23  Why Supermodel Carré Otis Is Taking on This To...   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://triblive.com/opinion/joseph-otis-minot...   \n",
       "1   https://www.fool.com/earnings/call-transcripts...   \n",
       "2   https://www.fool.com/earnings/call-transcripts...   \n",
       "3   https://www.intellasia.net/%EF%BB%BFotis-unvei...   \n",
       "4   https://www.intellasia.net/otis-unveils-new-ge...   \n",
       "6   https://www.marketscreener.com//quote/stock/OT...   \n",
       "7   https://www.centralmaine.com/2021/02/20/centra...   \n",
       "8   https://wwd.com/fashion-news/fashion-features/...   \n",
       "9   https://santamariatimes.com/lifestyles/today-i...   \n",
       "10  https://www.thedailybeast.com/supermodel-carre...   \n",
       "\n",
       "                                                 body  rel  hit_count  \\\n",
       "0   Fracking machinery at a site operated by Range...  228          9   \n",
       "1   Good morning, and welcome to Otis's First Quar...  162          1   \n",
       "2   Good morning and welcome to Otis Fourth Quarte...  136          1   \n",
       "3   - Gen3 elevator adds built-in IoT benefits to ...   78          1   \n",
       "4   - Gen3 elevator adds built-in IoT benefits to ...   78          1   \n",
       "6   FARMINGTON, Conn., June 8, 2021 /PRNewswire/ -...   78          1   \n",
       "7   JAY -- OTIS Federal Credit Union raised a tota...   52          4   \n",
       "8   Students graduating from the fashion program a...   27          2   \n",
       "9   Today is Thursday, April 29, the 119th day of ...   27          2   \n",
       "10  Carré Otis wants justice, but she also wants c...    2          2   \n",
       "\n",
       "   probable_esg_type company_name  article_id  \n",
       "0                all         otis           0  \n",
       "1                all         otis           1  \n",
       "2                all         otis           2  \n",
       "3                all         otis           3  \n",
       "4                all         otis           4  \n",
       "6                all         otis           5  \n",
       "7                all         otis           6  \n",
       "8                all         otis           7  \n",
       "9                all         otis           8  \n",
       "10               all         otis           9  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_cleaned_text(text):\n",
    "    text = re.sub(\"\\(.*?\\)\", '', text)\n",
    "    text = re.sub(\"\\[.*?\\]\", '', text)\n",
    "    text = re.sub('\\*', '', text)\n",
    "    text = re.sub('\\s\\s+', ' ', text)\n",
    "    text = re.sub('\\.\\s\\.', '.', text)\n",
    "    text = re.sub(',\\s,', ',', text)\n",
    "    text = re.sub(';\\s;', ';', text)\n",
    "    text = re.sub(':\\s:', ':', text)\n",
    "    text = re.sub('\\?\\s\\?', '?', text)\n",
    "    text = re.sub('\\!\\s\\!', '!', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_sents_for_doc(text):\n",
    "    text = get_cleaned_text(text)\n",
    "    sents = nltk.tokenize.sent_tokenize(str(text))\n",
    "    len_sents = len(sents)\n",
    "    final_2sents_list = []\n",
    "    for i in range(1, len_sents):\n",
    "        temp_text = sents[i-1] + ' ' + sents[i]\n",
    "        temp_text = re.sub('\"', '', temp_text)\n",
    "        final_2sents_list.append(temp_text.strip())\n",
    "    return final_2sents_list\n",
    "\n",
    "def map_snippets_art(row):\n",
    "    body = row['body']\n",
    "    snippets = get_sents_for_doc(body)\n",
    "    final_snippend_list = []\n",
    "    for snip in snippets:\n",
    "        temp_dict = dict()\n",
    "        temp_dict['text'] = snip\n",
    "        temp_dict['article_id'] = row['article_id']\n",
    "        temp_dict['url'] = row['url']\n",
    "        temp_dict['company_name'] = row['company_name']\n",
    "        temp_dict['title'] = row['title']\n",
    "        final_snippend_list.append(temp_dict)\n",
    "    return final_snippend_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_snippets(art_df, start_snippet_id):\n",
    "    snippets_list = []\n",
    "    for idx, row in tqdm(art_df.iterrows()):\n",
    "        one_snippet_list = map_snippets_art(row)\n",
    "        snippets_list.extend(one_snippet_list)\n",
    "    snippet_df = pd.DataFrame(snippets_list)\n",
    "    snippet_df['id'] = range(start_snippet_id, len(snippet_df))\n",
    "    return snippet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets_list = []\n",
    "lst=[]\n",
    "for idx, row in data_new2.iterrows():\n",
    "    lst.append(idx)\n",
    "    one_snippet_list = map_snippets_art(row)\n",
    "    snippets_list.extend(one_snippet_list)\n",
    "snippet_df = pd.DataFrame(snippets_list)\n",
    "snippet_df['id'] = range(0, len(snippet_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(snippet_df.id)\n",
    "#id_list=lst\n",
    "for i in range(1,len(id_list)):\n",
    "    if(id_list[i]-id_list[i-1]!=1):\n",
    "        print(id_list[i],id_list[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_df.to_csv('snippets_bytitle_V1_NLTK.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>url</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fracking machinery at a site operated by Range...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Environmental Protection Agency is on...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Next month, Sen. Martin Heinrich, D-N.M., will...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was the first ever federal standard for m...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We need President Biden and EPA Administrator ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  article_id  \\\n",
       "0  Fracking machinery at a site operated by Range...           0   \n",
       "1  The U.S. Environmental Protection Agency is on...           0   \n",
       "2  Next month, Sen. Martin Heinrich, D-N.M., will...           0   \n",
       "3  This was the first ever federal standard for m...           0   \n",
       "4  We need President Biden and EPA Administrator ...           0   \n",
       "\n",
       "                                                 url company_name  \\\n",
       "0  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "1  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "2  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "3  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "4  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "\n",
       "                                               title  id  \n",
       "0  Joseph Otis Minott: Biden must prioritize heal...   0  \n",
       "1  Joseph Otis Minott: Biden must prioritize heal...   1  \n",
       "2  Joseph Otis Minott: Biden must prioritize heal...   2  \n",
       "3  Joseph Otis Minott: Biden must prioritize heal...   3  \n",
       "4  Joseph Otis Minott: Biden must prioritize heal...   4  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new2.to_csv('data_bytitle_V2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4133/764195945.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  snippet_df = snippet_df.drop('id', 1)\n"
     ]
    }
   ],
   "source": [
    "snippet_df = snippet_df.drop('id', 1)\n",
    "snippet_df = snippet_df.sort_values('text')\n",
    "snippet_df = snippet_df.drop_duplicates('text')\n",
    "snippet_df = snippet_df.sort_values('article_id')\n",
    "snippet_df['id'] = range(0, len(snippet_df))\n",
    "snippet_df = snippet_df.sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(snippet_df.id)\n",
    "#id_list=lst\n",
    "for i in range(1,len(id_list)):\n",
    "    if(id_list[i]-id_list[i-1]!=1):\n",
    "        print(id_list[i],id_list[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>url</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>If we are to meet Biden's goal of a 100% clean...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Wind and solar must be the centerpieces of Bid...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>When methane is emitted from the oil and gas i...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Since every step in the oil and gas production...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>This includes aggressively expanding renewable...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  article_id  \\\n",
       "20  If we are to meet Biden's goal of a 100% clean...           0   \n",
       "30  Wind and solar must be the centerpieces of Bid...           0   \n",
       "12  When methane is emitted from the oil and gas i...           0   \n",
       "18  Since every step in the oil and gas production...           0   \n",
       "21  This includes aggressively expanding renewable...           0   \n",
       "\n",
       "                                                  url company_name  \\\n",
       "20  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "30  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "12  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "18  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "21  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "\n",
       "                                                title  id  \n",
       "20  Joseph Otis Minott: Biden must prioritize heal...   0  \n",
       "30  Joseph Otis Minott: Biden must prioritize heal...   1  \n",
       "12  Joseph Otis Minott: Biden must prioritize heal...   2  \n",
       "18  Joseph Otis Minott: Biden must prioritize heal...   3  \n",
       "21  Joseph Otis Minott: Biden must prioritize heal...   4  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'article_id', 'url', 'company_name', 'title', 'id'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_df.to_csv('snippets_bytitle_V1_after_DupRem_NLTK.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_cleaned_text(text):\n",
    "    text = re.sub(\"\\(.*?\\)\", '', text)\n",
    "    text = re.sub(\"\\[.*?\\]\", '', text)\n",
    "    text = re.sub('\\*', '', text)\n",
    "    text = re.sub('\\s\\s+', ' ', text)\n",
    "    text = re.sub('\\.\\s\\.', '.', text)\n",
    "    text = re.sub(',\\s,', ',', text)\n",
    "    text = re.sub(';\\s;', ';', text)\n",
    "    text = re.sub(':\\s:', ':', text)\n",
    "    text = re.sub('\\?\\s\\?', '?', text)\n",
    "    text = re.sub('\\!\\s\\!', '!', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_sents_for_doc(text):\n",
    "    text = get_cleaned_text(text)\n",
    "    #doc = nlp_spacy(text)\n",
    "    #sents = list(doc.sents)\n",
    "    sents = nltk.tokenize.sent_tokenize(str(text))\n",
    "    len_sents = len(sents)\n",
    "    final_1sents_list = []\n",
    "    for i in range(0, len_sents):\n",
    "        temp_text = sents[i]\n",
    "        temp_text = re.sub('\"', '', temp_text)\n",
    "        final_1sents_list.append(temp_text.strip())\n",
    "    return final_1sents_list\n",
    "\n",
    "def map_snippets_art(row):\n",
    "    body = row['body']\n",
    "    snippets = get_sents_for_doc(body)\n",
    "    final_snippend_list = []\n",
    "    for snip in snippets:\n",
    "        temp_dict = dict()\n",
    "        temp_dict['text'] = snip\n",
    "        temp_dict['article_id'] = row['article_id']\n",
    "        temp_dict['url'] = row['url']\n",
    "        temp_dict['company_name'] = row['company_name']\n",
    "        temp_dict['title'] = row['title']\n",
    "        final_snippend_list.append(temp_dict)\n",
    "    return final_snippend_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99302, 5)\n",
      "(98374, 5)\n"
     ]
    }
   ],
   "source": [
    "snippets_list = []\n",
    "lst=[]\n",
    "for idx, row in data_new2.iterrows():\n",
    "    lst.append(idx)\n",
    "    one_snippet_list = map_snippets_art(row)\n",
    "    snippets_list.extend(one_snippet_list)\n",
    "snippet_df = pd.DataFrame(snippets_list)\n",
    "#snippet_df = snippet_df.drop('id', 1)\n",
    "#snippet_df = snippet_df.sort_values('text')\n",
    "print(snippet_df.shape)\n",
    "snippet_df = snippet_df.drop_duplicates(['text','article_id'])\n",
    "snipper_df = snippet_df.sort_values('article_id')\n",
    "print(snippet_df.shape)\n",
    "#snippet_df['id'] = range(0, len(snippet_df))\n",
    "#snippet_df = snippet_df.sort_values('id')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>url</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fracking machinery at a site operated by Range...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Environmental Protection Agency is on...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Next month, Sen. Martin Heinrich, D-N.M., will...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was the first ever federal standard for m...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We need President Biden and EPA Administrator ...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  article_id  \\\n",
       "0  Fracking machinery at a site operated by Range...           0   \n",
       "1  The U.S. Environmental Protection Agency is on...           0   \n",
       "2  Next month, Sen. Martin Heinrich, D-N.M., will...           0   \n",
       "3  This was the first ever federal standard for m...           0   \n",
       "4  We need President Biden and EPA Administrator ...           0   \n",
       "\n",
       "                                                 url company_name  \\\n",
       "0  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "1  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "2  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "3  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "4  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "\n",
       "                                               title  \n",
       "0  Joseph Otis Minott: Biden must prioritize heal...  \n",
       "1  Joseph Otis Minott: Biden must prioritize heal...  \n",
       "2  Joseph Otis Minott: Biden must prioritize heal...  \n",
       "3  Joseph Otis Minott: Biden must prioritize heal...  \n",
       "4  Joseph Otis Minott: Biden must prioritize heal...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>url</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fracking machinery at a site operated by Range...</td>\n",
       "      <td>0</td>\n",
       "      <td>https://triblive.com/opinion/joseph-otis-minot...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Joseph Otis Minott: Biden must prioritize heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good morning, and welcome to Otis's First Quar...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.fool.com/earnings/call-transcripts...</td>\n",
       "      <td>otis</td>\n",
       "      <td>OTIS WORLDWIDE CORP (OTIS) Q1 2021 Earnings Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good morning and welcome to Otis Fourth Quarte...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.fool.com/earnings/call-transcripts...</td>\n",
       "      <td>otis</td>\n",
       "      <td>OTIS WORLDWIDE CORP (OTIS) Q4 2020 Earnings Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Gen3 elevator adds built-in IoT benefits to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.intellasia.net/%EF%BB%BFotis-unvei...</td>\n",
       "      <td>otis</td>\n",
       "      <td>﻿ Otis Unveils New Generation of Digitally Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Gen3 elevator adds built-in IoT benefits to ...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.intellasia.net/otis-unveils-new-ge...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Otis Unveils New Generation of Digitally Nativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FARMINGTON, Conn., June 8, 2021 /PRNewswire/ -...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.marketscreener.com//quote/stock/OT...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Otis Worldwide : Unveils New Generation of Dig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JAY -- OTIS Federal Credit Union raised a tota...</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.centralmaine.com/2021/02/20/centra...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Central Maine business briefs: OTIS Federal Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Students graduating from the fashion program a...</td>\n",
       "      <td>7</td>\n",
       "      <td>https://wwd.com/fashion-news/fashion-features/...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Otis College of Art and Design's Fashion Thesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Today is Thursday, April 29, the 119th day of ...</td>\n",
       "      <td>8</td>\n",
       "      <td>https://santamariatimes.com/lifestyles/today-i...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Today in History: Aretha Franklin's cover of O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Carré Otis wants justice, but she also wants c...</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.thedailybeast.com/supermodel-carre...</td>\n",
       "      <td>otis</td>\n",
       "      <td>Why Supermodel Carré Otis Is Taking on This To...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  article_id  \\\n",
       "0  Fracking machinery at a site operated by Range...           0   \n",
       "1  Good morning, and welcome to Otis's First Quar...           1   \n",
       "2  Good morning and welcome to Otis Fourth Quarte...           2   \n",
       "3  - Gen3 elevator adds built-in IoT benefits to ...           3   \n",
       "4  - Gen3 elevator adds built-in IoT benefits to ...           4   \n",
       "5  FARMINGTON, Conn., June 8, 2021 /PRNewswire/ -...           5   \n",
       "6  JAY -- OTIS Federal Credit Union raised a tota...           6   \n",
       "7  Students graduating from the fashion program a...           7   \n",
       "8  Today is Thursday, April 29, the 119th day of ...           8   \n",
       "9  Carré Otis wants justice, but she also wants c...           9   \n",
       "\n",
       "                                                 url company_name  \\\n",
       "0  https://triblive.com/opinion/joseph-otis-minot...         otis   \n",
       "1  https://www.fool.com/earnings/call-transcripts...         otis   \n",
       "2  https://www.fool.com/earnings/call-transcripts...         otis   \n",
       "3  https://www.intellasia.net/%EF%BB%BFotis-unvei...         otis   \n",
       "4  https://www.intellasia.net/otis-unveils-new-ge...         otis   \n",
       "5  https://www.marketscreener.com//quote/stock/OT...         otis   \n",
       "6  https://www.centralmaine.com/2021/02/20/centra...         otis   \n",
       "7  https://wwd.com/fashion-news/fashion-features/...         otis   \n",
       "8  https://santamariatimes.com/lifestyles/today-i...         otis   \n",
       "9  https://www.thedailybeast.com/supermodel-carre...         otis   \n",
       "\n",
       "                                               title  \n",
       "0  Joseph Otis Minott: Biden must prioritize heal...  \n",
       "1  OTIS WORLDWIDE CORP (OTIS) Q1 2021 Earnings Ca...  \n",
       "2  OTIS WORLDWIDE CORP (OTIS) Q4 2020 Earnings Ca...  \n",
       "3  ﻿ Otis Unveils New Generation of Digitally Nat...  \n",
       "4  Otis Unveils New Generation of Digitally Nativ...  \n",
       "5  Otis Worldwide : Unveils New Generation of Dig...  \n",
       "6  Central Maine business briefs: OTIS Federal Cr...  \n",
       "7  Otis College of Art and Design's Fashion Thesi...  \n",
       "8  Today in History: Aretha Franklin's cover of O...  \n",
       "9  Why Supermodel Carré Otis Is Taking on This To...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_list = []\n",
    "grouped_df = snippet_df.groupby(['article_id'])\n",
    "for name,group in grouped_df:\n",
    "    temp_dict = dict()\n",
    "    text = \"\\n\".join(list(group.text))\n",
    "    temp_dict['text'] = text\n",
    "    temp_dict['article_id'] = group.iloc[0].article_id\n",
    "    temp_dict['url'] = group.iloc[0].url\n",
    "    temp_dict['company_name'] = group.iloc[0].company_name\n",
    "    temp_dict['title'] = group.iloc[0].title\n",
    "    ind_list.append(temp_dict)\n",
    "drop_dup_df=pd.DataFrame(ind_list)\n",
    "drop_dup_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dup_df.to_csv('data_bytitle_V2_after_DupRem_NLTK.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_cleaned_text(text):\n",
    "    text = re.sub(\"\\(.*?\\)\", '', text)\n",
    "    text = re.sub(\"\\[.*?\\]\", '', text)\n",
    "    text = re.sub('\\*', '', text)\n",
    "    text = re.sub('\\s\\s+', ' ', text)\n",
    "    text = re.sub('\\.\\s\\.', '.', text)\n",
    "    text = re.sub(',\\s,', ',', text)\n",
    "    text = re.sub(';\\s;', ';', text)\n",
    "    text = re.sub(':\\s:', ':', text)\n",
    "    text = re.sub('\\?\\s\\?', '?', text)\n",
    "    text = re.sub('\\!\\s\\!', '!', text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_sents_for_doc(text):\n",
    "    text = get_cleaned_text(text)\n",
    "    #doc = nlp_spacy(text)\n",
    "    #sents = list(doc.sents)\n",
    "    sents = nltk.tokenize.sent_tokenize(str(text))\n",
    "    len_sents = len(sents)\n",
    "    final_2sents_list = []\n",
    "    for i in range(1, len_sents):\n",
    "        temp_text = sents[i-1] + ' ' + sents[i]\n",
    "        temp_text = re.sub('\"', '', temp_text)\n",
    "        final_2sents_list.append(temp_text.strip())\n",
    "    return final_2sents_list\n",
    "\n",
    "def map_snippets_art(row):\n",
    "    body = row['text']\n",
    "    snippets = get_sents_for_doc(body)\n",
    "    final_snippend_list = []\n",
    "    for snip in snippets:\n",
    "        temp_dict = dict()\n",
    "        temp_dict['text'] = snip\n",
    "        temp_dict['article_id'] = row['article_id']\n",
    "        temp_dict['url'] = row['url']\n",
    "        temp_dict['company_name'] = row['company_name']\n",
    "        temp_dict['title'] = row['title']\n",
    "        final_snippend_list.append(temp_dict)\n",
    "    return final_snippend_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets_list = []\n",
    "lst=[]\n",
    "for idx, row in drop_dup_df.iterrows():\n",
    "    lst.append(idx)\n",
    "    one_snippet_list = map_snippets_art(row)\n",
    "    snippets_list.extend(one_snippet_list)\n",
    "snippet_df = pd.DataFrame(snippets_list)\n",
    "snippet_df['id'] = range(0, len(snippet_df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_df.to_csv('snippet_bytitle_V2_after_DupRem_NLTK.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "26\n",
    "27\n",
    "28\n",
    "29\n",
    "30\n",
    "31\n",
    "32\n",
    "33\n",
    "34\n",
    "35\n",
    "36\n",
    "37\n",
    "38\n",
    "39\n",
    "40\n",
    "41\n",
    "42\n",
    "43\n",
    "44\n",
    "45\n",
    "46\n",
    "47\n",
    "48\n",
    "49\n",
    "50\n",
    "51\n",
    "52\n",
    "53\n",
    "54\n",
    "55\n",
    "56\n",
    "57\n",
    "58\n",
    "59\n",
    "60\n",
    "61\n",
    "62\n",
    "63\n",
    "64\n",
    "65\n",
    "66\n",
    "67\n",
    "68\n",
    "69\n",
    "70\n",
    "71\n",
    "72\n",
    "73\n",
    "74\n",
    "75\n",
    "76\n",
    "77\n",
    "78\n",
    "79\n",
    "80\n",
    "81\n",
    "82\n",
    "83\n",
    "84\n",
    "85\n",
    "86\n",
    "87\n",
    "88\n",
    "89\n",
    "90\n",
    "91\n",
    "92\n",
    "93\n",
    "94\n",
    "95\n",
    "96\n",
    "97\n",
    "98\n",
    "99\n",
    "100\n",
    "101\n",
    "102\n",
    "103\n",
    "104\n",
    "105\n",
    "106\n",
    "107\n",
    "108\n",
    "109\n",
    "110\n",
    "111\n",
    "112\n",
    "113\n",
    "114\n",
    "115\n",
    "116\n",
    "117\n",
    "118\n",
    "119\n",
    "120\n",
    "121\n",
    "122\n",
    "123\n",
    "124\n",
    "125\n",
    "126\n",
    "127\n",
    "128\n",
    "129\n",
    "130\n",
    "131\n",
    "132\n",
    "133\n",
    "134\n",
    "135\n",
    "136\n",
    "137\n",
    "138\n",
    "139\n",
    "140\n",
    "141\n",
    "142\n",
    "143\n",
    "144\n",
    "145\n",
    "146\n",
    "147\n",
    "148\n",
    "149\n",
    "150\n",
    "151\n",
    "152\n",
    "153\n",
    "154\n",
    "155\n",
    "156\n",
    "157\n",
    "158\n",
    "159\n",
    "160\n",
    "161\n",
    "162\n",
    "163\n",
    "164\n",
    "165\n",
    "166\n",
    "167\n",
    "168\n",
    "169\n",
    "170\n",
    "171\n",
    "172\n",
    "173\n",
    "174\n",
    "175\n",
    "176\n",
    "177\n",
    "178\n",
    "179\n",
    "181\n",
    "182\n",
    "183\n",
    "184\n",
    "185\n",
    "186\n",
    "187\n",
    "188\n",
    "190\n",
    "191\n",
    "192\n",
    "193\n",
    "194\n",
    "195\n",
    "196\n",
    "197\n",
    "198\n",
    "199\n",
    "200\n",
    "201\n",
    "202\n",
    "203\n",
    "204\n",
    "205\n",
    "206\n",
    "207\n",
    "208\n",
    "209\n",
    "210\n",
    "211\n",
    "212\n",
    "213\n",
    "214\n",
    "215\n",
    "216\n",
    "217\n",
    "218\n",
    "219\n",
    "220\n",
    "221\n",
    "222\n",
    "223\n",
    "224\n",
    "225\n",
    "226\n",
    "227\n",
    "228\n",
    "229\n",
    "230\n",
    "231\n",
    "232\n",
    "233\n",
    "234\n",
    "235\n",
    "236\n",
    "237\n",
    "238\n",
    "239\n",
    "240\n",
    "241\n",
    "242\n",
    "243\n",
    "244\n",
    "245\n",
    "246\n",
    "247\n",
    "248\n",
    "249\n",
    "250\n",
    "251\n",
    "252\n",
    "254\n",
    "255\n",
    "256\n",
    "257\n",
    "258\n",
    "259\n",
    "260\n",
    "261\n",
    "262\n",
    "263\n",
    "264\n",
    "265\n",
    "266\n",
    "267\n",
    "268\n",
    "269\n",
    "270\n",
    "271\n",
    "272\n",
    "273\n",
    "274\n",
    "275\n",
    "276\n",
    "277\n",
    "278\n",
    "279\n",
    "280\n",
    "281\n",
    "282\n",
    "283\n",
    "284\n",
    "285\n",
    "286\n",
    "287\n",
    "288\n",
    "289\n",
    "290\n",
    "291\n",
    "292\n",
    "293\n",
    "294\n",
    "295\n",
    "296\n",
    "297\n",
    "298\n",
    "299\n",
    "300\n",
    "301\n",
    "302\n",
    "303\n",
    "304\n",
    "305\n",
    "306\n",
    "307\n",
    "308\n",
    "309\n",
    "310\n",
    "311\n",
    "312\n",
    "313\n",
    "314\n",
    "315\n",
    "316\n",
    "317\n",
    "318\n",
    "319\n",
    "320\n",
    "321\n",
    "322\n",
    "323\n",
    "324\n",
    "325\n",
    "326\n",
    "327\n",
    "328\n",
    "329\n",
    "330\n",
    "331\n",
    "332\n",
    "333\n",
    "334\n",
    "335\n",
    "336\n",
    "337\n",
    "338\n",
    "339\n",
    "340\n",
    "341\n",
    "342\n",
    "343\n",
    "344\n",
    "345\n",
    "346\n",
    "347\n",
    "348\n",
    "349\n",
    "350\n",
    "351\n",
    "352\n",
    "353\n",
    "354\n",
    "355\n",
    "356\n",
    "357\n",
    "358\n",
    "359\n",
    "360\n",
    "361\n",
    "362\n",
    "363\n",
    "364\n",
    "365\n",
    "366\n",
    "367\n",
    "368\n",
    "369\n",
    "370\n",
    "371\n",
    "372\n",
    "373\n",
    "374\n",
    "375\n",
    "376\n",
    "377\n",
    "378\n",
    "380\n",
    "381\n",
    "382\n",
    "383\n",
    "384\n",
    "385\n",
    "386\n",
    "387\n",
    "388\n",
    "389\n",
    "390\n",
    "391\n",
    "392\n",
    "393\n",
    "394\n",
    "395\n",
    "396\n",
    "397\n",
    "398\n",
    "399\n",
    "400\n",
    "401\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
